@misc{Guo-reduccion,
title = {\url{https://github.com/ruiguo-bio/midi-miner}}
}


@inproceedings{Cuthbert2010Music21AT,
  title={Music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data},
  author={Mike Cuthbert and Christopher Ariza},
  booktitle={ISMIR},
  year={2010}
}

@article{shannon1948,
  title={A Mathematical Theory of Communication},
  author={Shannon, C.E.},
  journal={The Bell System Technical Journal},
  volume={27},
  year={1948}
}

@INPROCEEDINGS{WangDubnov2015,
  author={Wang, Cheng-i and Dubnov, Shlomo},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title={Pattern discovery from audio recordings by Variable Markov Oracle: A music information dynamics approach},
  year={2015},
  volume={},
  number={},
  pages={683-687},
  doi={10.1109/ICASSP.2015.7178056}
}

@article{lattner2018,
  title={Imposing higher-level structure in polyphonic music generation using convolutional restricted Boltzmann machines and constraints},
  author={Lattner, S. and Grachten, M. and Widmer, G.},
  journal={Journal of Creative Music Systems},
  volume={2},
  year={2018}
}


@article{zivic2013,
  title={Perceptual basis of evolving Western musical styles},
  author={Rodríguez Zivic, P. and Shifres, F. and Cecchi, G.},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  volume={110},
  year={2013}
}

@article{benetatos2020bachduet,
  title={Bachduet: A deep learning system for human-machine counterpoint improvisation},
  author={Benetatos, Christodoulos and VanderStel, Joseph and Duan, Zhiyao},
  journal={Proceedings of the International Conference on New Interfaces for Musical Expression},
  year={2020}
}

@article{Berardinis,
  title={Modelling long- and short-term structure in symbolic music with attention and recurrence},
  author={de Berardinis, Jacopo and Barret, Samuel and Cangelosi, Angelo and Coutinho, Eduardo},
  journal={Joint Conference on AI Music Creativity}
}

@article{payne2019musenet,
  title={MuseNet},
  author={Payne, Christine},
  journal={OpenAI Blog},
  volume={3},
  year={2019}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@book{briot2020deep,
  title={Deep learning techniques for music generation},
  author={Briot, Jean-Pierre and Hadjeres, Ga{\"e}tan and Pachet, Fran{\c{c}}ois-David},
  volume={1},
  year={2020},
  publisher={Springer}
}

@article{guo2020variational,
  title={A variational autoencoder for music generation controlled by tonal tension},
  author={Guo, Rui and Simpson, Ivor and Magnusson, Thor and Kiefer, Chris and Herremans, Dorien},
  journal={arXiv preprint arXiv:2010.06230},
  year={2020}
}
@article{roberts2019musicvae,
  author    = {Adam Roberts and
               Jesse H. Engel and
               Colin Raffel and
               Curtis Hawthorne and
               Douglas Eck},
  title     = {A Hierarchical Latent Vector Model for Learning Long-Term Structure
               in Music},
  journal   = {Proceedings of the 35th International Conference on Machine Learning},
  volume    = {abs/1803.05428},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.05428},
  eprinttype = {arXiv},
  eprint    = {1803.05428},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-05428.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{stahlberg2020neural,
  title={Neural machine translation: A review},
  author={Stahlberg, Felix},
  journal={Journal of Artificial Intelligence Research},
  volume={69},
  pages={343--418},
  year={2020},
  doi={https://doi.org/10.1613/jair.1.12007}
}

@article{macavaney2019hate,
    doi = {10.1371/journal.pone.0221152},
    author = {MacAvaney, Sean AND Yao, Hao-Ren AND Yang, Eugene AND Russell, Katina AND Goharian, Nazli AND Frieder, Ophir},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Hate speech detection: Challenges and solutions},
    year = {2019},
    month = {08},
    volume = {14},
    url = {https://doi.org/10.1371/journal.pone.0221152},
    pages = {1-16},
    abstract = {As online content continues to grow, so does the spread of hate speech. We identify and examine challenges faced by online automatic approaches for hate speech detection in text. Among these difficulties are subtleties in language, differing definitions on what constitutes hate speech, and limitations of data availability for training and testing of these systems. Furthermore, many recent approaches suffer from an interpretability problem—that is, it can be difficult to understand why the systems make the decisions that they do. We propose a multi-view SVM approach that achieves near state-of-the-art performance, while being simpler and producing more easily interpretable decisions than neural methods. We also discuss both technical and practical challenges that remain for this task.},
    number = {8},

}

@Article{popel2020news,
	author={Popel, Martin
	and Tomkova, Marketa
	and Tomek, Jakub
	and Kaiser, {\L}ukasz
	and Uszkoreit, Jakob
	and Bojar, Ond{\v{r}}ej
	and {\v{Z}}abokrtsk{\'y}, Zden{\v{e}}k},
	title={Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals},
	journal={Nature Communications},
	year={2020},
	month={Sep},
	day={01},
	volume={11},
	number={1},
	pages={4381},
	abstract={The quality of human translation was long thought to be unattainable for computer translation systems. In this study, we present a deep-learning system, CUBBITT, which challenges this view. In a context-aware blind evaluation by human judges, CUBBITT significantly outperformed professional-agency English-to-Czech news translation in preserving text meaning (translation adequacy). While human translation is still rated as more fluent, CUBBITT is shown to be substantially more fluent than previous state-of-the-art systems. Moreover, most participants of a Translation Turing test struggle to distinguish CUBBITT translations from human translations. This work approaches the quality of human translation and even surpasses it in adequacy in certain circumstances.This suggests that deep learning may have the potential to replace humans in applications where conservation of meaning is the primary aim.},
	issn={2041-1723},
	doi={10.1038/s41467-020-18073-9},
	url={https://doi.org/10.1038/s41467-020-18073-9}
}

@article{sun2015ragtime,
    author={Sun, Felix},
    title={CeepHear – Composing and harmonizing music with neural networks},
    url={https://fephsun.github.io/2015/09/01/neural-music.html}
}


@InProceedings{hadjeres2017bach,
  title = 	 {{D}eep{B}ach: a Steerable Model for {B}ach Chorales Generation},
  author =       {Ga{\"e}tan Hadjeres and Fran{\c{c}}ois Pachet and Frank Nielsen},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1362--1371},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/hadjeres17a/hadjeres17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/hadjeres17a.html},
  abstract = 	 {This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach’s strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.}
}

@article{eck2002first,
  title={A first look at music composition using lstm recurrent neural networks},
  author={Eck, Douglas and Schmidhuber, Juergen},
  journal={Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale},
  volume={103},
  pages={48},
  year={2002},
  publisher={Google Scholar}
}

@InProceedings{makris2017bass,
author="Makris, Dimos
and Kaliakatsos-Papakostas, Maximos
and Karydis, Ioannis
and Kermanidis, Katia Lida",
editor="Boracchi, Giacomo
and Iliadis, Lazaros
and Jayne, Chrisina
and Likas, Aristidis",
title="Combining LSTM and Feed Forward Neural Networks for Conditional Rhythm Composition",
booktitle="Engineering Applications of Neural Networks",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="570--582",
abstract="Algorithmic music composition has long been in the spotlight of music information research and Long Short-Term Memory (LSTM) neural networks have been extensively used for this task. However, despite LSTM networks having proven useful in learning sequences, no methodology has been proposed for learning sequences conditional to constraints, such as given metrical structure or a given bass line. In this paper we examine the task of conditional rhythm generation of drum sequences with Neural Networks. The proposed network architecture is a combination of LSTM and feed forward (conditional) layers capable of learning long drum sequences, under constraints imposed by metrical rhythm information and a given bass sequence. The results indicate that the role of the conditional layer in the proposed architecture is crucial for creating diverse drum sequences under conditions concerning given metrical information and bass lines.",
isbn="978-3-319-65172-9"
}

@misc{yang2017midinet,
  doi = {10.48550/ARXIV.1703.10847},
  url = {https://arxiv.org/abs/1703.10847},
  author = {Yang, Li-Chia and Chou, Szu-Yu and Yang, Yi-Hsuan},
  keywords = {Sound (cs.SD), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{mao2018deepj,
	doi = {10.1109/icsc.2018.00077},
	url = {https://doi.org/10.1109%2Ficsc.2018.00077},
	year = 2018,
	month = {jan},
	publisher = {{IEEE}},
	author = {Huanru Henry Mao and Taylor Shin and Garrison Cottrell},
	title = {{DeepJ}: Style-Specific Music Generation},
	booktitle = {2018 {IEEE} 12th International Conference on Semantic Computing ({ICSC})}
}

@misc{gatys2015style,
  doi = {10.48550/ARXIV.1508.06576},
  url = {https://arxiv.org/abs/1508.06576},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), Neurons and Cognition (q-bio.NC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
  title = {A Neural Algorithm of Artistic Style},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{upchurch2016feature,
  doi = {10.48550/ARXIV.1611.05507},
  url = {https://arxiv.org/abs/1611.05507},
  author = {Upchurch, Paul and Gardner, Jacob and Pleiss, Geoff and Pless, Robert and Snavely, Noah and Bala, Kavita and Weinberger, Kilian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Feature Interpolation for Image Content Changes},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{hou2016feature,
  doi = {10.48550/ARXIV.1610.00291},
  url = {https://arxiv.org/abs/1610.00291},
  author = {Hou, Xianxu and Shen, Linlin and Sun, Ke and Qiu, Guoping},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Feature Consistent Variational Autoencoder},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{luan2017photo,
  author    = {Fujun Luan and
               Sylvain Paris and
               Eli Shechtman and
               Kavita Bala},
  title     = {Deep Photo Style Transfer},
  journal   = {CoRR},
  volume    = {abs/1703.07511},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.07511},
  eprinttype = {arXiv},
  eprint    = {1703.07511},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LuanPSB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
